<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Random Sampling | turtleblog</title>
<link rel="stylesheet" href="/scss/main.min.c0bd8ef6f33e7379d872efa8e0bc6ea110ca8d9d400c9299c473bc78c8b757d0.css" integrity="sha256-wL2O9vM&#43;c3nYcu&#43;o4LxuoRDKjZ1ADJKZxHO8eMi3V9A=">

</head>
<body>
<main>
<header>
<div class="menu">
  <h1><a href="/">TURTLEBLOG</a></h1>
  <ul>
    <li><a href="/about.html">/ABOUT</a></li>
  </ul>
</div>

</header>

  <h1>Random Sampling</h1>
  
  
  <time datetime="2013-04-17T08:00:00&#43;08:00">April 17, 2013</time>
  <p>Given a set of items, choosing random one with equal probability can be done by <code>random.choice(items)</code>.
But what if we are given items one by one without knowing the length of the whole set?</p>
<p>There is a simple algorithm: For the $k$-th item, we give up previous selection and choose this one with probability $\frac 1 k$.
Proof as follows: If we select the $k$-th item, it means that we choose it with probability $\frac 1 k$ and give up all successors.
Product of probabilities of all these choices is:</p>
<p>$$ p_k = \frac 1 k \times \frac {k} {k+1} \times \cdots \times \frac {n-2}{n-1} \times \frac {n-1} n = \frac 1 n $$</p>
<p>Codes in Python:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">random</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">choice</span><span class="p">(</span><span class="n">items</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">selection</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">items</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">selection</span> <span class="o">=</span> <span class="n">item</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">selection</span>
</span></span></code></pre></div><p>We can generalize this algorithm. For a set of items, each one is associated with a weight $w_i$.
Our goal is to select a random one based on its weight ratio.
The algorithm is similar to the previous one: For the $k$-th item, replace the current selection with it with probability $\frac {w_k} {\sum_{i=1}^k w_i}$.
Proof is analogical:</p>
<p>$$p_k = \frac {w_k} {\sum_{i=1}^k w_i} \times \frac {\sum_{i=1}^k w_i} {\sum_{i=1}^{k+1} w_i}
\times \cdots \times \frac {\sum_{i=1}^{n-2} w_i} {\sum_{i=1}^{n-1} w_i} \times \frac {\sum_{i=1}^{n-1} w_i} {\sum_{i=1}^{n} w_i} = \frac {w_k} {\sum_{i=1}^n w_i}$$</p>
<p>Codes in Python:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">weightedChoice</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">selection</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="n">total_weight</span> <span class="o">=</span> <span class="mf">0.0</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">item</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">total_weight</span> <span class="o">+=</span> <span class="n">weight</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">*</span> <span class="n">total_weight</span> <span class="o">&lt;</span> <span class="n">weight</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">selection</span> <span class="o">=</span> <span class="n">item</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">selection</span>
</span></span></code></pre></div><p>There is another amazing method to do this weighted random sampling:
For each item, get a random $r_i \in [0, 1]$,
and <em>reweight</em> this item as $w&rsquo;_i = r_i^{1 / {w_i}}$.
Then we can select the one with the top new weight. Proof of this is a bit annoying.</p>
<p>If we choose the $i$-th item at last, this means $\forall j\neq i, w&rsquo;_j &lt;w&rsquo;_i$. As $r_i \in [0, 1]$, the probability is:</p>
<p>$$ p_i = \int_0^1 p(\forall j\neq i, w&rsquo;_j &lt;w&rsquo;_i)\ \mathrm{d}\ r_i $$</p>
<p>$r_j$ is independent with each other. So:</p>
<p>$$ p_i = \int_0^1 \prod_{j \neq i} p(w&rsquo;_j &lt;w&rsquo;_i)\ \mathrm{d} \ r_i $$</p>
<p>As $r_j \in [0, 1]$, the inner probability can be simplified as:</p>
<p>$$ p(w&rsquo;_j &lt;w&rsquo;_i) = p(r_j^{1 / w_j} &lt; r_i^{1 / w_i}) = p(r_j &lt; r_i^{w_j / w_i}) = r_i^{w_j / w_i} $$</p>
<p>So:</p>
<p>$$ p_i = \int_0^1 \prod_{j \neq i} r_i^{w_j / w_i}\ \mathrm{d}\ r_i
= \int_0^1 r_i^{(w-w_i) / w_i}\ \mathrm{d}\ r_i = \frac {w_i} w $$</p>
<p>Same as our expectation.</p>
<p>This blog is basically a summary of
<a href="http://www.gocalf.com/blog/random-selection.html">these</a>
<a href="http://www.gocalf.com/blog/weighted-random-selection.html">three</a>
<a href="http://www.gocalf.com/blog/weighted-random-selection-2.html">blogs</a>.
Refer to them for more content.</p>

  <script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]}}</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  

<footer>
<a href="https://github.com/scturtle"><svg><use xlink:href="/icons.svg#github"></use></svg></a>
<a href="https://twitter.com/scturtle"><svg><use xlink:href="/icons.svg#twitter"></use></svg></a>
<a href="https://t.me/scturtle"><svg><use xlink:href="/icons.svg#telegram"></use></svg></a>
<a href="/feed.xml"><svg><use xlink:href="/icons.svg#rss"></use></svg></a>

</footer>
</main>
</body>
</html>
